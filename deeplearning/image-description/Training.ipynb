{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战项目：图像标注\n",
    "\n",
    "---\n",
    "\n",
    "在这个notebook中，我们要做的是训练自己的循环神经网络模型。\n",
    "\n",
    "\n",
    "### 任务 #1\n",
    "\n",
    "首先，请设置以下变量：\n",
    "- `batch_size` - 每个训练批次的批次大小。它是指用于在每个训练步骤中修改模型权重的图像标注对的数量。\n",
    "- `vocab_threshold` - 单词阈值最小值。请注意，阈值越大，词汇量越小，而阈值越小，则表示将包括较少的词汇，词汇量则越大。\n",
    "- `vocab_from_file` - 一个布尔值，用于决定是否从文件加载词汇表。\n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  图像和单词嵌入的维度。\n",
    "- `hidden_size` - RNN解码器隐藏状态下的特征数。\n",
    "- `num_epochs` - 训练模型的epoch数。\n",
    "- `save_every` - 确定保存模型权重的频率。我们建议你设置为`save_every=1`，便于在每个epoch后保存模型权重。这样，在第`i`个epoch之后，编码器和解码器权重将在`models/`文件夹中分别保存为`encoder-i.pkl`和`decoder-i.pkl`。\n",
    "- `print_every` - 确定在训练时将批次损失输出到Jupyter notebook的频率。\n",
    "- `log_file` - 包含每个步骤中训练期间的损失与复杂度演变过程的的文本文件的名称。\n",
    "\n",
    "\n",
    "### 问题1\n",
    "\n",
    "**问题:** 详细描述你的CNN-RNN架构。对于这种架构，任务1中变量的值，你是如何选择的？如果你查阅了某一篇详细说明关于成功实现图像标注生成模型的研究论文，请提供该参考论文。\n",
    "\n",
    "**答案:** \n",
    "- 我参考了[ShowandTell: ANeuralImageCaptionGenerator](https://arxiv.org/pdf/1411.4555.pdf)这篇论文,使用LSTM这个来处理这个序列问题。适用dropout层来进一步抑制过拟合。我先把输入数据，使用Embedding层进行词嵌入，将整个描述说明序列的每个单词转换到相同维数，然后将input样本输入到LSTM得到每次处理的隐藏状态，即输出，再传给一个全连接层，输出的个数是我们统计所有单词的个数，因为我们对每个单词的输出做客独热编码。在参考了论文后，我设置batch_size是64，vocab_threshold是5，embed_size取256，hidden_size是512.采取的具体策略是编码器使用的CNN，对图片进行了编码，然后再将CNN处理的图片进行转换，使它的维度和embed_size一样，都是256维度。将这个图片编码的结果作为解码器的第一个输入，它预测的结果是描述序列的第一个词，也就是START字符,，然后第二个输入就是描述序列的一个词，预测结果是序列的第二个词，依次类推。最后的输入是倒数第二个词，预测结果是END字符，表示结束。\n",
    "\n",
    "\n",
    "### 问题2\n",
    "\n",
    "**问题:** 你是如何在`transform_train`中选择转换方式的？如果你将转换保留为其提供的值，为什么你的任务它非常适合你的CNN架构？\n",
    "\n",
    "**答案:** \n",
    "- 我使用transform操作是先把图片调整到256大小，因为有的图片大于256，有的图片小于256，我们使用的预训练模型需要224x224的长宽，我先将他们调整到256大小，然后将他们沿着中心裁剪为224X224，再使用一个随机水平反转，确保图片位置的多样性。组后对他们做归一化，就是减去mean再除以std，即转换为标准正态分布。使用CNN是因为卷积网络非常适合对图片进行特征提取，他对图片的通道和空间信息都有很好的过滤，我们使用一个优良的CNN架构，这就保证了模型的深度，使得我们可以对一个图片从边缘识别的简单提取，随着深度的增加可以提取到图片各部位的更加抽象高级的特征，比如人脸，一开始我们这提取轮廓边缘的特征，到后边可以提取到眉毛眼睛等高级特征，而CNN具有的平移不变性符合我们人眼视觉的特点。所以CNN非常适合提取图片特征，用CNN作为我们的解码器是合适的\n",
    "### 任务 #3\n",
    "\n",
    "接下来，你需要定义一个包含模型的可学习参数的Python列表。 例如，如果你决定使解码器中的所有权重都是可训练的，但只想在编码器的嵌入层中训练权重，那么，就应该将`params`设置为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题3\n",
    "\n",
    "**问题:** 你是如何选择该架构的可训练参数的？ 为什么你认为这是一个不错的选择？\n",
    "\n",
    "**答案:** \n",
    "- 首先解码器阶段，我们使用迁移学习的方式来提取图片特征，也就是使用预训练模型，这极大提升了我们的工作效率，使我们可以省去大部分训练的时间。这种预训练的模型是经过长期验证得到的高效优良模型，我们使用resnet预训练模型非常适合处理图像问题，和我们的需求很相似。我们没必要再重新训练模型。而通过预训练模型得到结果，并不能满足我们的需求，我们需要对其做一个Embedding嵌入操作，使得和我们的描述序列的每个词都是同样的维度吗，因此我们需要一个全连接层将其转换为相同的embed维度。这个全连接层是需要我们训练的。而解码器中是我们自己定义的模型架构，因此需要我们对其相关的所有权重参数进行训练。这样的可以得到一个比较满意的模型\n",
    "\n",
    "\n",
    "### 问题4\n",
    "\n",
    "**问题:** 你是如何选择用于训练模型的优化程序的？\n",
    "\n",
    "**答案:**\n",
    "首先我们使用dropout，dropout可以按概率随机将部分隐含层节点的权重归零，由于每次迭代受归零影响的节点不同，因此各节点的重要性会被平衡，这其实是个平均化思想，不要把一直放在权重较高的节点上，同时也要照顾到较低的权重。然后使用adam优化器算法，它结合AdaGrad和RMSProp两种优化算法的优点，很适合应用于大规模的数据及参数的场景和适用于梯度稀疏或梯度存在很大噪声的问题。目前基本都是用adam作为默认优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.51s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [01:18<00:00, 5302.49it/s]\n",
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "batch_size = 64          \n",
    "vocab_threshold = 5      \n",
    "vocab_from_file = True   \n",
    "embed_size = 256    #词嵌入维度       \n",
    "hidden_size = 512   #状态变量维度\n",
    "num_epochs = 3             \n",
    "save_every = 1             \n",
    "print_every = 100          \n",
    "log_file = 'training_log.txt' \n",
    "\n",
    "#用于数据增强\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.RandomCrop(224),                      \n",
    "    transforms.RandomHorizontalFlip(),               \n",
    "    transforms.ToTensor(),                          \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "#词汇数量\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "#初始化编码器和解码器 \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "#如果有gpu并且支持cuda，可以使用gpu训练，否则使用cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "#定义损失函数 \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "#定义哪些参数需要参与训练.编码器的embed词嵌入层和解码器的所有层参数\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "#定义adam优化器\n",
    "optimizer = optim.Adam(params, lr=0.001)\n",
    "\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: 训练你的模型\n",
    "\n",
    "在**Step 1**中执行代码单元格后，就可以进行下面的训练模型的阶段了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/6471], Loss: 4.3183, Perplexity: 75.0597\n",
      "Epoch [1/3], Step [200/6471], Loss: 3.8799, Perplexity: 48.42068\n",
      "Epoch [1/3], Step [300/6471], Loss: 3.4509, Perplexity: 31.52904\n",
      "Epoch [1/3], Step [400/6471], Loss: 3.4717, Perplexity: 32.1924\n",
      "Epoch [1/3], Step [500/6471], Loss: 3.7226, Perplexity: 41.37222\n",
      "Epoch [1/3], Step [600/6471], Loss: 3.1691, Perplexity: 23.78584\n",
      "Epoch [1/3], Step [700/6471], Loss: 3.3633, Perplexity: 28.88402\n",
      "Epoch [1/3], Step [800/6471], Loss: 3.0314, Perplexity: 20.7269\n",
      "Epoch [1/3], Step [900/6471], Loss: 3.1502, Perplexity: 23.3405\n",
      "Epoch [1/3], Step [1000/6471], Loss: 3.2243, Perplexity: 25.1352\n",
      "Epoch [1/3], Step [1100/6471], Loss: 3.0306, Perplexity: 20.7102\n",
      "Epoch [1/3], Step [1200/6471], Loss: 2.7712, Perplexity: 15.9782\n",
      "Epoch [1/3], Step [1300/6471], Loss: 2.7818, Perplexity: 16.1474\n",
      "Epoch [1/3], Step [1400/6471], Loss: 2.8917, Perplexity: 18.0247\n",
      "Epoch [1/3], Step [1500/6471], Loss: 3.2716, Perplexity: 26.3545\n",
      "Epoch [1/3], Step [1600/6471], Loss: 2.8333, Perplexity: 17.0020\n",
      "Epoch [1/3], Step [1700/6471], Loss: 2.7218, Perplexity: 15.2069\n",
      "Epoch [1/3], Step [1800/6471], Loss: 3.3745, Perplexity: 29.2100\n",
      "Epoch [1/3], Step [1900/6471], Loss: 2.9613, Perplexity: 19.3227\n",
      "Epoch [1/3], Step [2000/6471], Loss: 2.6008, Perplexity: 13.4742\n",
      "Epoch [1/3], Step [2100/6471], Loss: 2.8409, Perplexity: 17.1318\n",
      "Epoch [1/3], Step [2200/6471], Loss: 2.9913, Perplexity: 19.91123\n",
      "Epoch [1/3], Step [2300/6471], Loss: 2.8960, Perplexity: 18.1023\n",
      "Epoch [1/3], Step [2400/6471], Loss: 2.5594, Perplexity: 12.9282\n",
      "Epoch [1/3], Step [2500/6471], Loss: 2.7181, Perplexity: 15.1510\n",
      "Epoch [1/3], Step [2600/6471], Loss: 2.8904, Perplexity: 18.0013\n",
      "Epoch [1/3], Step [2700/6471], Loss: 2.6794, Perplexity: 14.5765\n",
      "Epoch [1/3], Step [2800/6471], Loss: 2.6512, Perplexity: 14.1713\n",
      "Epoch [1/3], Step [2900/6471], Loss: 3.4807, Perplexity: 32.4826\n",
      "Epoch [1/3], Step [3000/6471], Loss: 2.4097, Perplexity: 11.1310\n",
      "Epoch [1/3], Step [3100/6471], Loss: 2.5010, Perplexity: 12.1947\n",
      "Epoch [1/3], Step [3200/6471], Loss: 2.5662, Perplexity: 13.0158\n",
      "Epoch [1/3], Step [3300/6471], Loss: 2.5732, Perplexity: 13.1080\n",
      "Epoch [1/3], Step [3400/6471], Loss: 2.5631, Perplexity: 12.9766\n",
      "Epoch [1/3], Step [3500/6471], Loss: 2.7406, Perplexity: 15.4967\n",
      "Epoch [1/3], Step [3600/6471], Loss: 2.7599, Perplexity: 15.7981\n",
      "Epoch [1/3], Step [3700/6471], Loss: 2.6593, Perplexity: 14.2859\n",
      "Epoch [1/3], Step [3800/6471], Loss: 2.4485, Perplexity: 11.5707\n",
      "Epoch [1/3], Step [3900/6471], Loss: 2.8241, Perplexity: 16.8452\n",
      "Epoch [1/3], Step [4000/6471], Loss: 2.5610, Perplexity: 12.9486\n",
      "Epoch [1/3], Step [4100/6471], Loss: 2.7114, Perplexity: 15.0501\n",
      "Epoch [1/3], Step [4200/6471], Loss: 2.3231, Perplexity: 10.2077\n",
      "Epoch [1/3], Step [4300/6471], Loss: 2.4662, Perplexity: 11.7774\n",
      "Epoch [1/3], Step [4400/6471], Loss: 2.6594, Perplexity: 14.2882\n",
      "Epoch [1/3], Step [4500/6471], Loss: 2.6437, Perplexity: 14.0655\n",
      "Epoch [1/3], Step [4600/6471], Loss: 3.1700, Perplexity: 23.8082\n",
      "Epoch [1/3], Step [4700/6471], Loss: 3.0569, Perplexity: 21.2606\n",
      "Epoch [1/3], Step [4800/6471], Loss: 2.8811, Perplexity: 17.8344\n",
      "Epoch [1/3], Step [4900/6471], Loss: 2.5372, Perplexity: 12.6444\n",
      "Epoch [1/3], Step [5000/6471], Loss: 2.8118, Perplexity: 16.6391\n",
      "Epoch [1/3], Step [5100/6471], Loss: 2.5153, Perplexity: 12.3703\n",
      "Epoch [1/3], Step [5200/6471], Loss: 2.3743, Perplexity: 10.74339\n",
      "Epoch [1/3], Step [5300/6471], Loss: 2.4672, Perplexity: 11.7899\n",
      "Epoch [1/3], Step [5400/6471], Loss: 2.7839, Perplexity: 16.1818\n",
      "Epoch [1/3], Step [5500/6471], Loss: 2.7348, Perplexity: 15.4068\n",
      "Epoch [1/3], Step [5600/6471], Loss: 2.5994, Perplexity: 13.4562\n",
      "Epoch [1/3], Step [5700/6471], Loss: 2.1800, Perplexity: 8.84622\n",
      "Epoch [1/3], Step [5800/6471], Loss: 2.3667, Perplexity: 10.6622\n",
      "Epoch [1/3], Step [5900/6471], Loss: 2.4780, Perplexity: 11.9170\n",
      "Epoch [1/3], Step [6000/6471], Loss: 2.4404, Perplexity: 11.4779\n",
      "Epoch [1/3], Step [6100/6471], Loss: 2.5082, Perplexity: 12.2828\n",
      "Epoch [1/3], Step [6200/6471], Loss: 2.4583, Perplexity: 11.6848\n",
      "Epoch [1/3], Step [6300/6471], Loss: 2.1986, Perplexity: 9.01237\n",
      "Epoch [1/3], Step [6400/6471], Loss: 2.6430, Perplexity: 14.0559\n",
      "Epoch [2/3], Step [100/6471], Loss: 2.3708, Perplexity: 10.70551\n",
      "Epoch [2/3], Step [200/6471], Loss: 2.4249, Perplexity: 11.3006\n",
      "Epoch [2/3], Step [300/6471], Loss: 2.3410, Perplexity: 10.3920\n",
      "Epoch [2/3], Step [400/6471], Loss: 2.2081, Perplexity: 9.09871\n",
      "Epoch [2/3], Step [500/6471], Loss: 2.7247, Perplexity: 15.25229\n",
      "Epoch [2/3], Step [600/6471], Loss: 2.4115, Perplexity: 11.1506\n",
      "Epoch [2/3], Step [700/6471], Loss: 3.2103, Perplexity: 24.7858\n",
      "Epoch [2/3], Step [800/6471], Loss: 2.1545, Perplexity: 8.62335\n",
      "Epoch [2/3], Step [900/6471], Loss: 2.4257, Perplexity: 11.3100\n",
      "Epoch [2/3], Step [1000/6471], Loss: 2.3144, Perplexity: 10.1185\n",
      "Epoch [2/3], Step [1100/6471], Loss: 2.3682, Perplexity: 10.6780\n",
      "Epoch [2/3], Step [1200/6471], Loss: 2.7236, Perplexity: 15.2356\n",
      "Epoch [2/3], Step [1300/6471], Loss: 2.6536, Perplexity: 14.2044\n",
      "Epoch [2/3], Step [1400/6471], Loss: 2.3092, Perplexity: 10.0664\n",
      "Epoch [2/3], Step [1500/6471], Loss: 2.5144, Perplexity: 12.3588\n",
      "Epoch [2/3], Step [1600/6471], Loss: 2.2941, Perplexity: 9.91584\n",
      "Epoch [2/3], Step [1700/6471], Loss: 2.8533, Perplexity: 17.34483\n",
      "Epoch [2/3], Step [1800/6471], Loss: 2.2589, Perplexity: 9.57300\n",
      "Epoch [2/3], Step [1900/6471], Loss: 2.2781, Perplexity: 9.75817\n",
      "Epoch [2/3], Step [2000/6471], Loss: 2.4697, Perplexity: 11.8183\n",
      "Epoch [2/3], Step [2100/6471], Loss: 2.2968, Perplexity: 9.94240\n",
      "Epoch [2/3], Step [2200/6471], Loss: 2.5712, Perplexity: 13.0811\n",
      "Epoch [2/3], Step [2300/6471], Loss: 2.2926, Perplexity: 9.90044\n",
      "Epoch [2/3], Step [2400/6471], Loss: 2.5412, Perplexity: 12.6951\n",
      "Epoch [2/3], Step [2500/6471], Loss: 2.2811, Perplexity: 9.78727\n",
      "Epoch [2/3], Step [2600/6471], Loss: 2.2139, Perplexity: 9.15176\n",
      "Epoch [2/3], Step [2700/6471], Loss: 2.2910, Perplexity: 9.88446\n",
      "Epoch [2/3], Step [2800/6471], Loss: 2.5510, Perplexity: 12.8196\n",
      "Epoch [2/3], Step [2900/6471], Loss: 2.3206, Perplexity: 10.1816\n",
      "Epoch [2/3], Step [3000/6471], Loss: 2.5083, Perplexity: 12.2835\n",
      "Epoch [2/3], Step [3100/6471], Loss: 2.5631, Perplexity: 12.9753\n",
      "Epoch [2/3], Step [3200/6471], Loss: 2.3336, Perplexity: 10.3152\n",
      "Epoch [2/3], Step [3300/6471], Loss: 2.5188, Perplexity: 12.4139\n",
      "Epoch [2/3], Step [3400/6471], Loss: 2.3543, Perplexity: 10.5310\n",
      "Epoch [2/3], Step [3500/6471], Loss: 2.5771, Perplexity: 13.1592\n",
      "Epoch [2/3], Step [3600/6471], Loss: 2.2857, Perplexity: 9.83309\n",
      "Epoch [2/3], Step [3700/6471], Loss: 2.2058, Perplexity: 9.07720\n",
      "Epoch [2/3], Step [3800/6471], Loss: 2.2662, Perplexity: 9.64259\n",
      "Epoch [2/3], Step [3900/6471], Loss: 2.5889, Perplexity: 13.3151\n",
      "Epoch [2/3], Step [4000/6471], Loss: 2.4116, Perplexity: 11.1522\n",
      "Epoch [2/3], Step [4100/6471], Loss: 2.3834, Perplexity: 10.8420\n",
      "Epoch [2/3], Step [4200/6471], Loss: 2.3212, Perplexity: 10.1881\n",
      "Epoch [2/3], Step [4300/6471], Loss: 2.0884, Perplexity: 8.07235\n",
      "Epoch [2/3], Step [4400/6471], Loss: 2.2442, Perplexity: 9.43288\n",
      "Epoch [2/3], Step [4500/6471], Loss: 2.3256, Perplexity: 10.2325\n",
      "Epoch [2/3], Step [4600/6471], Loss: 2.4088, Perplexity: 11.1206\n",
      "Epoch [2/3], Step [4700/6471], Loss: 2.3772, Perplexity: 10.7748\n",
      "Epoch [2/3], Step [4800/6471], Loss: 2.3751, Perplexity: 10.7519\n",
      "Epoch [2/3], Step [4900/6471], Loss: 2.2575, Perplexity: 9.55906\n",
      "Epoch [2/3], Step [5000/6471], Loss: 2.1161, Perplexity: 8.29896\n",
      "Epoch [2/3], Step [5100/6471], Loss: 2.2667, Perplexity: 9.64773\n",
      "Epoch [2/3], Step [5200/6471], Loss: 2.9590, Perplexity: 19.2794\n",
      "Epoch [2/3], Step [5300/6471], Loss: 2.2383, Perplexity: 9.37769\n",
      "Epoch [2/3], Step [5400/6471], Loss: 2.3094, Perplexity: 10.0682\n",
      "Epoch [2/3], Step [5500/6471], Loss: 2.7193, Perplexity: 15.1693\n",
      "Epoch [2/3], Step [5600/6471], Loss: 2.3055, Perplexity: 10.0294\n",
      "Epoch [2/3], Step [5700/6471], Loss: 2.2491, Perplexity: 9.47950\n",
      "Epoch [2/3], Step [5800/6471], Loss: 2.2960, Perplexity: 9.93442\n",
      "Epoch [2/3], Step [5900/6471], Loss: 2.5589, Perplexity: 12.9215\n",
      "Epoch [2/3], Step [6000/6471], Loss: 2.5566, Perplexity: 12.8916\n",
      "Epoch [2/3], Step [6100/6471], Loss: 2.3866, Perplexity: 10.8762\n",
      "Epoch [2/3], Step [6200/6471], Loss: 2.5466, Perplexity: 12.7634\n",
      "Epoch [2/3], Step [6300/6471], Loss: 2.3012, Perplexity: 9.98592\n",
      "Epoch [2/3], Step [6400/6471], Loss: 2.2193, Perplexity: 9.201337\n",
      "Epoch [3/3], Step [100/6471], Loss: 2.2995, Perplexity: 9.969355\n",
      "Epoch [3/3], Step [200/6471], Loss: 2.5378, Perplexity: 12.6516\n",
      "Epoch [3/3], Step [300/6471], Loss: 2.2269, Perplexity: 9.27134\n",
      "Epoch [3/3], Step [400/6471], Loss: 2.1525, Perplexity: 8.60677\n",
      "Epoch [3/3], Step [500/6471], Loss: 2.3440, Perplexity: 10.4231\n",
      "Epoch [3/3], Step [600/6471], Loss: 2.4179, Perplexity: 11.2224\n",
      "Epoch [3/3], Step [700/6471], Loss: 2.3530, Perplexity: 10.5173\n",
      "Epoch [3/3], Step [800/6471], Loss: 2.4740, Perplexity: 11.8699\n",
      "Epoch [3/3], Step [900/6471], Loss: 2.2856, Perplexity: 9.83203\n",
      "Epoch [3/3], Step [1000/6471], Loss: 2.5242, Perplexity: 12.4811\n",
      "Epoch [3/3], Step [1100/6471], Loss: 2.5753, Perplexity: 13.1349\n",
      "Epoch [3/3], Step [1200/6471], Loss: 2.8908, Perplexity: 18.0073\n",
      "Epoch [3/3], Step [1300/6471], Loss: 2.3635, Perplexity: 10.6282\n",
      "Epoch [3/3], Step [1400/6471], Loss: 2.3053, Perplexity: 10.0273\n",
      "Epoch [3/3], Step [1500/6471], Loss: 2.2946, Perplexity: 9.92091\n",
      "Epoch [3/3], Step [1600/6471], Loss: 1.9793, Perplexity: 7.23780\n",
      "Epoch [3/3], Step [1700/6471], Loss: 2.3172, Perplexity: 10.1471\n",
      "Epoch [3/3], Step [1800/6471], Loss: 2.4644, Perplexity: 11.7560\n",
      "Epoch [3/3], Step [1900/6471], Loss: 2.3098, Perplexity: 10.0727\n",
      "Epoch [3/3], Step [2000/6471], Loss: 2.2027, Perplexity: 9.04952\n",
      "Epoch [3/3], Step [2100/6471], Loss: 2.2935, Perplexity: 9.90937\n",
      "Epoch [3/3], Step [2200/6471], Loss: 2.4568, Perplexity: 11.6669\n",
      "Epoch [3/3], Step [2300/6471], Loss: 2.5708, Perplexity: 13.0763\n",
      "Epoch [3/3], Step [2400/6471], Loss: 1.9885, Perplexity: 7.30433\n",
      "Epoch [3/3], Step [2500/6471], Loss: 2.9672, Perplexity: 19.4370\n",
      "Epoch [3/3], Step [2600/6471], Loss: 2.3261, Perplexity: 10.2375\n",
      "Epoch [3/3], Step [2700/6471], Loss: 2.5646, Perplexity: 12.9957\n",
      "Epoch [3/3], Step [2800/6471], Loss: 2.0823, Perplexity: 8.02283\n",
      "Epoch [3/3], Step [2900/6471], Loss: 2.3762, Perplexity: 10.7638\n",
      "Epoch [3/3], Step [3000/6471], Loss: 2.3996, Perplexity: 11.0183\n",
      "Epoch [3/3], Step [3100/6471], Loss: 2.1370, Perplexity: 8.47449\n",
      "Epoch [3/3], Step [3200/6471], Loss: 2.0679, Perplexity: 7.90839\n",
      "Epoch [3/3], Step [3300/6471], Loss: 2.2019, Perplexity: 9.04244\n",
      "Epoch [3/3], Step [3400/6471], Loss: 2.4531, Perplexity: 11.6238\n",
      "Epoch [3/3], Step [3500/6471], Loss: 2.4008, Perplexity: 11.0317\n",
      "Epoch [3/3], Step [3600/6471], Loss: 2.3094, Perplexity: 10.0685\n",
      "Epoch [3/3], Step [3700/6471], Loss: 2.2195, Perplexity: 9.20319\n",
      "Epoch [3/3], Step [3800/6471], Loss: 2.3363, Perplexity: 10.3424\n",
      "Epoch [3/3], Step [3900/6471], Loss: 2.3694, Perplexity: 10.6914\n",
      "Epoch [3/3], Step [4000/6471], Loss: 2.1170, Perplexity: 8.30605\n",
      "Epoch [3/3], Step [4100/6471], Loss: 2.4362, Perplexity: 11.4290\n",
      "Epoch [3/3], Step [4200/6471], Loss: 2.3666, Perplexity: 10.6609\n",
      "Epoch [3/3], Step [4300/6471], Loss: 2.5713, Perplexity: 13.0824\n",
      "Epoch [3/3], Step [4400/6471], Loss: 1.9616, Perplexity: 7.11055\n",
      "Epoch [3/3], Step [4500/6471], Loss: 2.3047, Perplexity: 10.0214\n",
      "Epoch [3/3], Step [4600/6471], Loss: 2.2732, Perplexity: 9.71041\n",
      "Epoch [3/3], Step [4700/6471], Loss: 2.4588, Perplexity: 11.6912\n",
      "Epoch [3/3], Step [4800/6471], Loss: 2.2563, Perplexity: 9.54765\n",
      "Epoch [3/3], Step [4900/6471], Loss: 2.0371, Perplexity: 7.66800\n",
      "Epoch [3/3], Step [5000/6471], Loss: 2.2862, Perplexity: 9.83743\n",
      "Epoch [3/3], Step [5100/6471], Loss: 2.3779, Perplexity: 10.7826\n",
      "Epoch [3/3], Step [5200/6471], Loss: 2.4140, Perplexity: 11.1791\n",
      "Epoch [3/3], Step [5300/6471], Loss: 2.5353, Perplexity: 12.62087\n",
      "Epoch [3/3], Step [5400/6471], Loss: 2.2596, Perplexity: 9.57962\n",
      "Epoch [3/3], Step [5500/6471], Loss: 2.2529, Perplexity: 9.51538\n",
      "Epoch [3/3], Step [5600/6471], Loss: 2.2617, Perplexity: 9.59915\n",
      "Epoch [3/3], Step [5700/6471], Loss: 3.0295, Perplexity: 20.6878\n",
      "Epoch [3/3], Step [5800/6471], Loss: 2.3034, Perplexity: 10.0085\n",
      "Epoch [3/3], Step [5900/6471], Loss: 2.1322, Perplexity: 8.43330\n",
      "Epoch [3/3], Step [6000/6471], Loss: 2.3855, Perplexity: 10.8649\n",
      "Epoch [3/3], Step [6100/6471], Loss: 2.2352, Perplexity: 9.34824\n",
      "Epoch [3/3], Step [6200/6471], Loss: 2.7338, Perplexity: 15.3917\n",
      "Epoch [3/3], Step [6300/6471], Loss: 2.1952, Perplexity: 8.98224\n",
      "Epoch [3/3], Step [6400/6471], Loss: 2.2664, Perplexity: 9.64518\n",
      "Epoch [3/3], Step [6457/6471], Loss: 2.3050, Perplexity: 10.0240"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#记录训练日志\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    h = decoder.init_hidden(64)\n",
    "    for i_step in range(1, total_step+1):\n",
    "      \n",
    "        # 获取序列长度等于某个随机值的样本索引.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # 根据获取的样本索引进行批量采样.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        #获得批量数据\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        #我们使用的是gpu训练。将数据交给cuda计算\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        h = tuple([each.data for each in h])\n",
    "        # 每次获取新的批量数据，要对之前的梯度清0\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        #将输入数据传给我们的编码器-解码器架构模型来处理.\n",
    "        features = encoder(images)\n",
    "        outputs, h = decoder(features, captions,h)\n",
    "        \n",
    "        #计算批量损失\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        #进行反向传播.\n",
    "        loss.backward()\n",
    "        \n",
    "        #通过优化器更新参数.\n",
    "        optimizer.step()\n",
    "            \n",
    "        #获取训练统计信息.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        #将信息写入日志文件\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # 保存权重，每save_every个间隔保存一次权重。分别保存编码器和解码器的权重。下次可以直接加载这些权重\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
