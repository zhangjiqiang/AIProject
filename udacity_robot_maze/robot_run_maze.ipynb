{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目概述\n",
    "\n",
    "## 1. 项目描述\n",
    "\n",
    "\n",
    "<img src=\"default.png\" width=\"20%\"></img>\n",
    "\n",
    "在该项目中，将使用强化学习算法，实现一个自动走迷宫机器人。\n",
    "\n",
    "1. 如上图所示，智能机器人显示在右上角。在我们的迷宫中，有陷阱（红色炸弹）及终点（蓝色的目标点）两种情景。机器人要尽量避开陷阱、尽快到达目的地。\n",
    "2. 小车可执行的动作包括：向上走 `u`、向右走 `r`、向下走 `d`、向左走 `l`。\n",
    "3. 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。\n",
    "    - 撞到墙壁：-10\n",
    "    - 走到终点：50\n",
    "    - 走到陷阱：-30\n",
    "    - 其余情况：-0.1\n",
    "4. 我们需要通过修改 `robot.py` 中的代码，来实现一个 Q Learning 机器人，实现上述的目标。\n",
    "\n",
    "## 2. 环境配置\n",
    "\n",
    "配置环境，使用 `envirnment.yml` 文件配置名为 `robot-env` 的 conda 环境，具体而言，你只需转到当前的目录，在命令行/终端中运行如下代码，稍作等待即可。\n",
    "```\n",
    "conda env create -f envirnment.yml\n",
    "```\n",
    "安装完毕后，在命令行/终端中运行 `source activate robot-env`（Mac/Linux 系统）或 `activate robot-env`（Windows 系统）激活该环境。\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  算法理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 1 强化学习总览\n",
    "\n",
    "强化学习作为机器学习算法的一种，其模式也是让智能体在“训练”中学到“经验”，以实现给定的任务。但不同于监督学习与非监督学习，在强化学习的框架中，我们更侧重通过智能体与环境的**交互**来学习。通常在监督学习和非监督学习任务中，智能体往往需要通过给定的训练集，辅之以既定的训练目标（如最小化损失函数），通过给定的学习算法来实现这一目标。然而在强化学习中，智能体则是通过其与环境交互得到的奖励进行学习。这个环境可以是虚拟的（如虚拟的迷宫），也可以是真实的（自动驾驶汽车在真实道路上收集数据）。\n",
    "\n",
    "\n",
    "在强化学习中有五个核心组成部分，它们分别是：**环境（Environment）**、**智能体（Agent）**、**状态（State）**、**动作（Action）**和**奖励（Reward）**。在某一时间节点 $t$：\n",
    "    \n",
    "- 智能体在从环境中感知其所处的状态 $s_t$\n",
    "- 智能体根据某些准则选择动作 $a_t$\n",
    "- 环境根据智能体选择的动作，向智能体反馈奖励 $r_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**问题 1**：请参照如上的定义，描述出 “机器人走迷宫这个问题” 中强化学习五个组成部分对应的实际对象：\n",
    "\n",
    "- **环境** : 当前虚拟迷宫,迷宫由陷阱比如炸弹，墙壁，终点，还有其余情况(正常的没有陷阱和不撞到墙的位置)\n",
    "- **状态** : 机器人所处迷宫的位置，也就是当前在迷宫的位置坐标\n",
    "\n",
    "\n",
    "- **动作** : 包含上,下,左,右四个方向的动作\n",
    "- **奖励** : 撞到墙壁：-10,走到终点：50,走到陷阱：-30,其余情况：-0.1\n",
    "         \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 计算 Q 值 \n",
    "\n",
    "在我们的项目中，我们要实现基于 Q-Learning 的强化学习算法。Q-Learning 是一个值迭代（Value Iteration）算法。与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。因此，对每个状态值的准确估计，是我们值迭代算法的核心。通常我们会考虑**最大化动作的长期奖励**，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。\n",
    "\n",
    "在 Q-Learning 算法中，我们把这个长期奖励记为 Q 值，我们会考虑每个 ”状态-动作“ 的 Q 值，具体而言，它的计算公式为：\n",
    "\n",
    "$$\n",
    "q(s_{t},a) = R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1})\n",
    "$$\n",
    "\n",
    "也就是对于当前的“状态-动作” $(s_{t},a)$，我们考虑执行动作 $a$ 后环境给我们的奖励 $R_{t+1}$，以及执行动作 $a$ 到达 $s_{t+1}$后，执行任意动作能够获得的最大的Q值 $\\max_a q(a,s_{t+1})$，$\\gamma$ 为折扣因子。\n",
    "\n",
    "不过一般地，我们使用更为保守地更新 Q 表的方法，即引入松弛变量 $alpha$，按如下的公式进行更新，使得 Q 表的迭代变化更为平缓。\n",
    "\n",
    "$$\n",
    "q(s_{t},a) = (1-\\alpha) \\times q(s_{t},a) + \\alpha \\times(R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "<img src=\"default2.png\" width=\"20%\"></img>\n",
    "\n",
    "**问题 2**：根据已知条件求 $q(s_{t},a)$，在如下模板代码中的空格填入对应的数字即可。\n",
    "\n",
    "\n",
    "已知：如上图，机器人位于 $s_1$，行动为 `u`，行动获得的奖励与题目的默认设置相同。在 $s_2$ 中执行各动作的 Q 值为：`u`: -24，`r`: -13，`d`: -0.29、`l`: +40，$\\gamma$ 取0.9。\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q(s_{t},a) & = R_{t+1} + \\gamma \\times\\max_a q(a,s_{t+1}) \\\\\n",
    " & =(-0.1) + (0.9)*(40) \\\\\n",
    " & =(35.9)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 如何选择动作\n",
    "\n",
    "在强化学习中，「探索-利用」问题是非常重要的问题。具体来说，根据上面的定义，我们会尽可能地让机器人在每次选择最优的决策，来最大化长期奖励。但是这样做有如下的弊端：\n",
    "1. 在初步的学习中，我们的 Q 值会不准确，如果在这个时候都按照 Q 值来选择，那么会造成错误。\n",
    "2. 学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。\n",
    "\n",
    "因此我们需要一种办法，来解决如上的问题，增加机器人的探索。由此我们考虑使用 epsilon-greedy 算法，即在小车选择动作的时候，以一部分的概率随机选择动作，以一部分的概率按照最优的 Q 值选择动作。同时，这个选择随机动作的概率应当随着训练的过程逐步减小。请看如下代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "actions = ['u','r','d','l']\n",
    "qline = {'u':1.2, 'r':-2.1, 'd':-24.5, 'l':27}\n",
    "epsilon = 0.3 # 以0.3的概率进行随机选择\n",
    "\n",
    "#实现1 随机概率\n",
    "def choose_action(epsilon):\n",
    "    \n",
    "    action = None\n",
    "    if random.uniform(0,1) < epsilon: # 以某一概率\n",
    "        action = random.choice(actions) # 实现对动作的随机选择\n",
    "    else: \n",
    "        action = max(qline, key=qline.get) # 否则选择具有最大 Q 值的动作\n",
    "    return action\n",
    "print(choose_action(epsilon))\n",
    "\n",
    "#实现2 使用epsilon-greedy算法\n",
    "def choose_action(epsilon):\n",
    "    action_dict = {}\n",
    "    index = 0\n",
    "    for action_item in actions:\n",
    "        action_dict[action_item] = index\n",
    "        index += 1\n",
    "    max_Q_action =  max(qline, key=qline.get)\n",
    "    policy_q = np.ones(len(actions)) * epsilon / len(actions)\n",
    "    policy_q[action_dict[max_Q_action]] = 1 - epsilon + epsilon / len(actions)\n",
    "    action = np.random.choice(actions, p=policy_q)\n",
    "    return action\n",
    "action = choose_action(0.3)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
